import requests
from bs4 import BeautifulSoup
import peewee as pw
import time

myDB = pw.MySQLDatabase(host='45.77.oo8.XXX', port=3306, user='root', passwd='123000o.', database='spider')


class MySQLModel(pw.Model):
    """A base model that will use our MySQL database"""

    class Meta:
        database = myDB


# å’Œæ•°æ®åº“è¡¨åç§°ç›¸å¯¹åº”
class QiuShi(MySQLModel):
    id = pw.AutoField
    username = pw.CharField()
    age = pw.IntegerField()
    sex = pw.CharField()
    praise_num = pw.IntegerField()
    comment_num = pw.IntegerField()
    content = pw.CharField()
    avatar = pw.CharField()


myDB.connect()

headers = {
    'user-agent': 'Mozilla/5.0'
}

for i in range(1, 11):

    url = 'http://www.qiushibaike.com/hot/page/' + str(i)
    print('Start crawling page:', url)
    try:
        html = requests.get(url, headers=headers)
        soup = BeautifulSoup(html.text, 'html.parser')
        # print('soup', soup)
        targets = soup.find_all('div', 'article')
        # print(targets)
        print(len(targets))
        data_source = []
        for item in targets:
            avatar = 'https:' + item.find('div', 'author').img['src']
            username = item.find('div', 'author').img['alt']
            # print(item.find('div', 'author').find('div', 'articleGender')['class'])
            # print(item.find('div', 'author').find('div', 'articleGender') is None)
            gender_tag = item.find('div', 'author').find('div', 'articleGender')
            if gender_tag is None:
                sex = 'None'
                age = 0
            elif gender_tag['class'][1] == 'womenIcon':
                sex = 'woman'
                age = gender_tag.string
            else:
                sex = 'man'
                age = gender_tag.string

            praise_num = 0 if item.find('span', 'stats-vote').i is None else item.find('span', 'stats-vote').i.string
            comment_num = 0 if item.find('span', 'stats-comments').i is None else item.find('span',
                                                                                            'stats-comments').i.string
            content = item.find('div', 'content').span.get_text().lstrip().rstrip()
            print
            data_source.append(
                {'avatar': avatar, 'username': username, 'age': age, 'sex': sex, 'praise_num': praise_num,
                 'comment_num': comment_num, 'content': content})

        print('data_source:', data_source)
        # æ’å…¥æ•°æ®åº“
        QiuShi.insert_many(data_source).execute()

    except requests.exceptions.ConnectionError:
        print('ConnectionError -- please wait 3 seconds')

    time.sleep(1)


# å‡ºç°çš„é—®é¢˜
# peewee.ProgrammingError: (1146, "Table 'spider.qiu' doesn't exist") éœ€è¦å’Œæ•°æ®åº“è¡¨åç›¸å¯¹åº”

# 'content': 'å¤šä¹ˆç—›çš„é¢†æ‚Ÿï¼Œè€å©†åˆšæ‹–å¥½çš„åœ°ï¼Œæˆ‘ç«Ÿç„¶å¿˜äº†ğŸ˜£ ğŸ˜£ğŸ˜£
# pymysql.err.InternalError: (1366, "Incorrect string value:
# '\\xF0\\x9F\\x98\\xA3 \\xF0...' for column 'content' at row 24")

# éœ€è¦ä¿®æ”¹è¡¨çš„ CHARSET=utf8 => CHARSET=utf8mb4
# ENGINE=InnoDB AUTO_INCREMENT=126 DEFAULT CHARSET=utf8 COMMENT='ç³—äº‹ç™¾


# é¢å‘å¯¹è±¡è®¾è®¡æ¨¡å¼
"""
class QSBK:

    # åˆå§‹åŒ–æ–¹æ³•ï¼Œå®šä¹‰ä¸€äº›å˜é‡
    def __init__(self):
        self.pageIndex = 1
        self.user_agent = 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'
        self.headers = {'User-Agent': self.user_agent}
        self.stories = []  # å­˜æ”¾æ®µå­çš„å˜é‡ï¼Œæ¯ä¸€ä¸ªå…ƒç´ æ˜¯æ¯ä¸€é¡µçš„æ®µå­ä»¬
        self.enable = False  # å­˜æ”¾ç¨‹åºæ˜¯å¦ç»§ç»­è¿è¡Œçš„å˜é‡

    # ä¼ å…¥æŸä¸€é¡µçš„ç´¢å¼•è·å¾—é¡µé¢ä»£ç 
    def get_page(self, page_index):
        try:
            qiu_url = 'http://www.qiushibaike.com/hot/page/' + str(page_index)
            o_html = requests.get(qiu_url, headers=self.headers)
            o_soup = BeautifulSoup(o_html.text, 'html.parser')
            return o_soup
        except requests.exceptions.ConnectionError:
            print('ConnectionError -- please wait 3 seconds')

    # è·å–é¡µé¢item
    def get_page_item(self, page_index):
        o_soup = self.get_page(page_index)
        print(o_soup)
        # è¿›è¡Œé¡µé¢å…ƒç´ çš„æå–æ“ä½œ...
        return o_soup

    # è‡ªåŠ¨åŠ è½½æ–°é¡µé¢
    def load_page(self):
        if self.enable:
            page_stories = self.get_page_item(self.pageIndex)
            if page_stories:
                self.stories.append(page_stories)
                self.pageIndex += 1

    # å¼€å§‹æ–¹æ³•
    def start(self):
        print(u"æ­£åœ¨è¯»å–ç³—äº‹ç™¾ç§‘,æŒ‰å›è½¦æŸ¥çœ‹æ–°æ®µå­ï¼ŒQé€€å‡º")
        # å…ˆåŠ è½½ä¸€é¡µå†…å®¹
        self.load_page()


spider = QSBK()
spider.start()
"""
